Here's a `README.md` file that describes the two scripts for the `pgvector` benchmark:

### `README.md`

```markdown
# pgvector Benchmarking

This repository contains two Python scripts for working with `pgvector` extension in PostgreSQL. The scripts help in creating tables, indexing vectors, and benchmarking query performance.

## Prerequisites

Before running the scripts, make sure you have the following:

1. **PostgreSQL** installed with `pgvector` extension enabled.
2. **Python** installed with the required packages (`psycopg2`, `numpy`, `sklearn`).

Install the required Python packages:

```bash
pip3 install psycopg2 numpy scikit-learn
```

## Scripts

### 1. `index_creation.py`

This script creates a table in PostgreSQL, inserts randomly generated vector data, and creates an index for efficient search. It performs the following:

- Creates the `pgvector` extension in PostgreSQL (if not already installed).
- Creates a table `embeddings` with an `id` and `embedding` column.
- Inserts `N` (default 10,000) random vectors of size `D` (default 128).
- Creates an HNSW index on the `embedding` column for cosine similarity search.

#### How to run:

1. Modify the `DB_CONFIG` with your PostgreSQL connection details.
2. Run the script to initialize the table and index:

```bash
python3 index_creation.py
```

### 2. `query_benchmark.py`

This script benchmarks the performance of querying vectors from the PostgreSQL `embeddings` table. It performs the following:

- Loads query vectors from a file (`query_vectors.txt`).
- Executes a search query for each vector using cosine similarity.
- Executes the queries concurrently (default: 100 users) and measures the response time.
- Computes the 90th, 95th, and 99th percentiles of query response times.

#### How to run:

1. **Generate Query Vectors**:
   The script requires `query_vectors.txt`, which should contain query vectors in CSV format. If you don't have this file, you can generate it using the following script or use your own vectors.

2. **Run the script** to benchmark query performance:

```bash
python3 query_benchmark.py
```

### Example: Generating Query Vectors

To generate query vectors, use the following code in a separate script or interactive session:

```python
import numpy as np
from sklearn.preprocessing import normalize

# Generate random query vectors
def generate_query_vectors(n, d):
    vectors = np.random.rand(n, d).astype(np.float32)
    return normalize(vectors)

query_vectors = generate_query_vectors(1000, 128)  # 1000 query vectors of dimension 128
np.savetxt("query_vectors.txt", query_vectors, delimiter=",")
```

This will create a `query_vectors.txt` file that can be used in `query_benchmark.py`.

## Configuration

You can modify the following parameters in both scripts:

- `D` (Embedding size, default: 128)
- `N` (Number of vectors, default: 10,000)
- `QUERY_VECTOR_COUNT` (Number of query vectors, default: 1,000)
- `NUM_CONCURRENT_QUERIES` (Number of concurrent queries, default: 100)

Make sure to adjust these parameters based on your system's capacity and testing requirements.

## License

This repository is open source. Feel free to use it and modify it according to your needs.

```

### Explanation:

- **index_creation.py**: This script sets up the PostgreSQL table, generates random vectors, and indexes them.
- **query_benchmark.py**: This script performs a benchmark on querying the vectors, showing query performance statistics like the 90th, 95th, and 99th percentiles.
  
You can add this `README.md` to your project repository, providing the necessary information for users to set up and run the scripts.

Let me know if you'd like any modifications!